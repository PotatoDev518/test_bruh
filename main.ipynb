{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74023a3a-7352-4641-99fb-8eb421ea70b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langid in c:\\users\\sz74s\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.1.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\sz74s\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from langid) (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install langid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76fdd6a2-fe26-43cd-bc73-d404807d6372",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import langid\n",
    "import tensorflow as tf\n",
    "\n",
    "from random import randint as ran\n",
    "from math import floor\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c02e65d8-6a5d-47f9-a420-a3226820df0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls = []\n",
    "used_urls = []\n",
    "text_File = open(\"text.txt\",\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f4319e-3324-4535-9368-37075160995b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_text(url):\n",
    "  global used_urls\n",
    "\n",
    "  temp_urls = []\n",
    "  texts = []\n",
    "  try:\n",
    "    content = requests.get(url)\n",
    "    #print(url)\n",
    "  except:\n",
    "    return []\n",
    "  soup = BeautifulSoup(requests.get( url ).content, 'html.parser')\n",
    "  used_urls.append(url)\n",
    "\n",
    "  # get text from soup\n",
    "  for para in soup.findAll(\"p\"):\n",
    "    text = para.get_text()\n",
    "    text = \" \".join(text.split())\n",
    "  # add to a list and then write list to file so we can use the \"not in\" checks\n",
    "    if text not in texts and text not in [\"\",\" \"] and langid.classify(text)[0] == \"en\":\n",
    "      texts.append(text)\n",
    "    for t in texts:\n",
    "      if t != \"\" and len(t.split()) > len(\"five_\") and t[-1] != \":\":\n",
    "        try:\n",
    "            text_File.write(t + \"\\n\")\n",
    "        except:\n",
    "            pass\n",
    "            #print(\"cant write\")\n",
    "    texts = []\n",
    "\n",
    "  # get urls\n",
    "  for link in soup.findAll(\"a\"):\n",
    "    # get the link and onwards from the text\n",
    "    linkText = str(link)\n",
    "    startIndex = linkText.find(\"href\")\n",
    "\n",
    "    # if has link\n",
    "    if startIndex != -1:\n",
    "      link_and_stuff = linkText[startIndex + len('href=\"')::]\n",
    "      \n",
    "  # if link belongs to wikipedia, \n",
    "  #we add the wikilink because paths are relative, \n",
    "  #in theory we could do this by code with every website\n",
    "  # but im lazy\n",
    "      # link_and_stuff.startswith('/wiki/')\n",
    "      if link_and_stuff.startswith('/wiki/'):\n",
    "        WIKILINK = \"https://en.wikipedia.org\"\n",
    "        link_and_stuff = WIKILINK + link_and_stuff\n",
    "      \n",
    "      # remove the onwards from the text\n",
    "      endIndex = 0\n",
    "\n",
    "      if link_and_stuff[0:len(\"http\")] != \"http\":\n",
    "        pass\n",
    "      elif \"'\" in link_and_stuff:\n",
    "        endIndex = link_and_stuff.find(\"'\")\n",
    "      elif '\"' in link_and_stuff:\n",
    "        endIndex = link_and_stuff.find('\"')\n",
    "\n",
    "      url_ = link_and_stuff[0:endIndex]\n",
    "      if url_ != \"\" and url_ not in used_urls and url_ not in temp_urls and url_ not in urls:\n",
    "        temp_urls.append(url_)\n",
    "  \n",
    "  return temp_urls\n",
    "\n",
    "def main():\n",
    "  urls = [\"https://en.wikipedia.org/wiki/Main_Page\"]\n",
    "  THRESHOLDINGB = 3.5\n",
    "\n",
    "  while True:\n",
    "    fileSize = float(os.path.getsize(\"text.txt\")  / (1024 ** 3)) # in bytes so convert to gb\n",
    "    #print(str(floor(fileSize * 10000)/10000) + \"GB\") #filesize formatted\n",
    "\n",
    "    if len(urls) == 0:\n",
    "      urls.append(\"https://en.wikipedia.org/wiki/Special:Random\")\n",
    "\n",
    "    if fileSize < THRESHOLDINGB:\n",
    "      for i in range(32):\n",
    "        \n",
    "        #print(\"urls\",len(urls))\n",
    "        _ = get_text(urls[0])\n",
    "\n",
    "        if len(urls) < 50:\n",
    "          urls.extend(_)\n",
    "        else:\n",
    "            pass  \n",
    "            #print(\"urls too large\")\n",
    "\n",
    "        urls.pop(0)\n",
    "\n",
    "    else:\n",
    "      print(\"File Done!\")\n",
    "      return\n",
    "\n",
    "  print(\"used urls:\\n      \",used_urls)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
